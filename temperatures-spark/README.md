# Tasks
* Implement a time series analysis on top of some historical weather data. 
* Implement a method in Scala 

Both tasks are required.

# Time series analysis
The purpose of this task is to model a data sink on a data storage of your choice to allow time series analysis and populate it with a substantial amount of daily weather data (Manageable on a single machine) from National Climate Data Center. Once the data is in place carry out an experimental time series analysis to get some descriptive statistics about a trend of climate change. 

##### Specification
+ A NoSQL table (Cassandra, HBase, DynamoDB) that captures location, timestamp, min temp and max temp.
+ Parallel computation tool (Java MapReduce, Pig, Cascading, Spark etc..) 




# SOLUTION


## TASK 1 Steps:
  
### CASSANDRA SETUP:

1. Download the input data:

	ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/
     
        In our case the input data used is the annual/2015.
        The format contains 8 fields. Only the 4 first are used: station_id, date, type_value, value
        The type_value fields contains TMAX, TMIN, PRCP, SNOW...

2) Download Cassandra database

3) Execute cassandra database:
```
    ./cassandra
```
4) Execute Cassandra client shell: 
```
./cqlsh --request-timeout 50000
```
5) Create the table that will store the weather records:
```
CREATE KEYSPACE AddBrain WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3};
USE AddBrain;

CREATE TABLE WEATHER (
  id_station varchar,
  date timestamp,
  type varchar,
  value  int,
  flag1 varchar,
  flag2 varchar,
  flag3 varchar,
  flag4 varchar,
  PRIMARY KEY (id_station, date, type)
);
```
6)Migrate the data from the csv file to the cassandra table:
```
COPY WEATHER FROM '/home/dave/workspace/2015.csv';
```

### SBT SETUP
1) It is required sbt to run the project. 

2) In the project directory type:
```
	sbt clean compile test package
```

### SPARK SETUP
1) Download spark distribution. In my case I am using spark 1.6.2 (the one I had installed).

2) Download the spark cassandra connector
	        https://spark-packages.org/package/datastax/spark-cassandra-connector
	
3)Copy the jar dependency generated by SBT (addbrain_2.10-1.0.jar) on the target folder to the ${SPARK_HOME}/bin


3) Execute spark:
```
	sudo ./spark-shell  --jars spark-cassandra-connector-1.6.1-s_2.10.jar,spark-temperatures_2.10-1.0.jar --packages datastax:spark-cassandra-connector:1.6.0-s_2.10 --conf spark.cassandra.connection.host=localhost
```
4) From the spark shell write the following commands:
```
scala> import com.addbrain.SparkWeatherDao

scala> SparkWeatherDao.getLowerMinimumTemperatures(4)
res1: List[com.addbrain.WeatherRecord] = List(WeatherRecord(USS0033J01S,Thu Jan 01 06:35:50 GMT 1970,TMIN,-999), WeatherRecord(USS0050K05S,Thu Jan 01 06:35:50 GMT 1970,TMIN,-999), WeatherRecord(USS0050K07S,Thu Jan 01 06:35:50 GMT 1970,TMIN,-999), WeatherRecord(USS0050M01S,Thu Jan 01 06:35:50 GMT 1970,TMIN,-999))

scala> SparkWeatherDao.getUpperMinimumTemperatures(5)
res2: List[com.addbrain.WeatherRecord] = List(WeatherRecord(USR0000NHIG,Thu Jan 01 06:35:50 GMT 1970,TMIN,600), WeatherRecord(USR0000NHIG,Thu Jan 01 06:35:50 GMT 1970,TMIN,600), WeatherRecord(USR0000NHIG,Thu Jan 01 06:35:50 GMT 1970,TMIN,600), WeatherRecord(USR0000NHIG,Thu Jan 01 06:35:50 GMT 1970,TMIN,600), WeatherRecord(USR0000NHIG,Thu Jan 01 06:35:50 GMT 1970,TMIN,600))


scala> SparkWeatherDao.getAverageMinimumTemperatures
res0: Double = 57.0 

scala> SparkWeatherDao.getAverageMaxTemperatures
res1: Double = 173.0  
```


The results related average temperatures are correct as they are measured as tenths of celsius degrees:

    http://earthscience.stackexchange.com/questions/5015/what-is-celsius-degrees-to-tenths
The result should be divided by 10. That means the minimum temperatures average is 5.7 ÂºC and the maximum temperatures average during the year is 17.3. This value has sense.

### Comments about the code: 

It has been created a simple interface with some base operations about weather:
```
trait WeatherDao {
  def getUpperMinimumTemperatures(numberRecords: Int): List[WeatherRecord]

  def getLowerMinimumTemperatures(numberRecords: Int): List[WeatherRecord]

  def getAverageMinimumTemperatures(): Double

  def getUpperMaxTemperatures(numberRecords: Int): List[WeatherRecord]

  def getLowerMaxTemperatures(numberRecords: Int): List[WeatherRecord]

  def getAverageMaxTemperatures(): Double
}
```
This functionality could be increased with more complex operations over the current data.

### TODO
* Add test for the SparkWeatherDao service. To have a perfect simulation the idea would be to start an embedded cassandra server and populate the database before the tests start.
  A possibility is to include the phantom dependency:
  
            http://outworkers.com/blog/post/a-series-on-phantom-part-1-getting-started-with-phantom
  
* Add more functions to the weather service, like grouping by date and stationId. I already tried grouping by stationID, but as the suffle of the GroupBy takes so many resources, i had a memory leak. 
  It is always prefer to avoid groupBy, until it is required and the resultant RDD is smaller than the initial RDD.
* Add sbt assembly plugin. Right now the sbt package it only includes the compilation of the scala classes and resources, but the dependencies are not attached. For running this jar on spark, it is required to
  add the cassandra spark connector dependency to the spark classpath.


